{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5508e5c5",
   "metadata": {},
   "source": [
    "### Program to scrape NFL stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4952b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import html5lib as htlib\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as requests\n",
    "import re\n",
    "import urllib.parse\n",
    "from urllib.error import HTTPError\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61f0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filepath(path, tablename):\n",
    "    \n",
    "    # make a filepath for a cvs file to stare information\n",
    "    filepath = Path(path + tablename.replace('.', '') + '.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e659f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table(url, tablename, path):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # get a list of all tables on a webpage\n",
    "        tables = pd.read_html(url)\n",
    "\n",
    "        # data is now a list of dataframes (spreadsheets) one dataframe for each table in the page\n",
    "        \n",
    "    except HTTPError as err:\n",
    "        \n",
    "        # if wrong web address print the address for manual review\n",
    "        if err.code  == 404:\n",
    "        \n",
    "            print(url + '404')\n",
    "            \n",
    "        # if other HTTP error print the website in question for inspection\n",
    "        else:\n",
    "            print(tablename + str(err.code))\n",
    "\n",
    "            filepath = make_filepath(path, tablename)\n",
    "        \n",
    "    # otherwise save the data tables\n",
    "    else:\n",
    "        n = len(tables)        \n",
    "        i=0\n",
    "        \n",
    "        for i < n:\n",
    "            filepath = path + i + tablename.strip(\".\") + .csv\n",
    "            tables[i].to_csv(filepath)\n",
    "            i = i+1\n",
    "        \n",
    "        return tables   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46980dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ea6a4e5e25ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tables' is not defined"
     ]
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5d51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
       "\n",
    "# read the team name information\n",
    "allTeams = pd.read_csv('~\all_teams.csv')\n",
    "\n",
    "# websites to scrape\n",
    "pfr = 'https://www.pro-football-reference.com/years/2022/'\n",
    "sonny = 'https://sonnymoorepowerratings.com/nfl-foot.htm'\n",
    "fbinside = 'https://www.footballoutsiders.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e8a87",
   "metadata": {},
   "source": [
    "First, collect data from https://www.pro-football-reference.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d422a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# list of sudirectories for tables on nfl reference\n",
    "tablenames = ['opp.htm', 'games.htm', 'index.htm', '#passing', '#all_rushing', '#all_returns', '#all_kicking', '#all_team_scoring', '#all_team_conversions', '#all_drives']\n",
    "\n",
    "AllpfrDF = pd.DataFrame()\n",
    "AllpfrDF = pd.concat([AllpfrDF,allTeams], axis = 0)\n",
    "# iterate through the tables to create dataframes\n",
    "for tablename in tablenames:\n",
    "    \n",
    "    # assign the url\n",
    "    url = pfr + tablename\n",
    "    \n",
    "    filename = tablename\n",
    "    \n",
    "    # scrape the tables\n",
    "    tablelist = scrape_table(url, tablename, path)\n",
    "    print(len(tablelist))\n",
    "    AllpfrDF = pd.concat([AllpfrDF, tablelist], axis = 1)\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9346b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllpfrDF = pd.DataFrame()\n",
    "AllpfrDF = pd.concat([AllpfrDF, pd.read_csv('C:/Users/sarae/Desktop/NFL_Modeling/games.htm.csv')], axis = 0)\n",
    "allpfrDF = AllpfrDF.merge(allTeams, how='left', on='full_team_name')\n",
    "AllpfrDF = AllpfrDF.merge('~/opp.htm.csv'), how='left', on='full_team_name')\n",
    "AllpfrDF = AllpfrDF.merge('~/soonymoore.csv'), how='left', on='full_team_name')\n",
    "AllpfrDF = AllpfrDF.merge('~/dvoa_off.csv'),how='left', on='full_team_name')\n",
    "\n",
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16f6a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oti429\n",
      "was429\n"
     ]
    }
   ],
   "source": [
    "# select team abbreviations\n",
    "tablenames = allTeams[\"pfr_abbreviation\"]\n",
    "teamAbb = allTeams[\"team_abbreviation\"]\n",
    "\n",
    "#iterate through teamAbb to scrape data\n",
    "for tablename in tablenames:\n",
    "    \n",
    "    # assign the url\n",
    "    url = 'https://www.pro-football-reference.com/teams/' + tablename + '/2022_lines.htm#vegas_lines' \n",
    " \n",
    "    # give each a unique df name\n",
    "    filename = 'df_vegasline_' + tablename\n",
    "    \n",
    "    # scrape the tables\n",
    "    scrape_table(url, tablename, path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88e759",
   "metadata": {},
   "source": [
    "Second, collect data from https://sonnymoorepowerratings.com/nfl-foot.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b5220226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the sonny more table\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n",
    "}\n",
    "html = requests.get(sonny,headers=headers)\n",
    "soup = Soup(html.content, 'html.parser').body.center.pre.font\n",
    "#souptag = soup.body.center.pre.font\n",
    "\n",
    "filepath = make_filepath(path, 'sonnymoore')\n",
    "              \n",
    "strtable = soup.getText()\n",
    "\n",
    "table = strtable.split()\n",
    "\n",
    "# this leaves us with a string of values that still need to be made into a table     \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d24d69",
   "metadata": {},
   "source": [
    "Third, collect data from https://www.footballoutsiders.com/stats/nfl/team-efficiency/2022/regular\n",
    "\n",
    "Site is password protected without an account, only 5 lines show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16174410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pro-football-reference.com/years/2022//stats/nfl/team-defense/2020404\n",
      "https://www.pro-football-reference.com/years/2022//stats/nfl/team-offense/2020404\n",
      "https://www.pro-football-reference.com/years/2022//stats/nfl/team-efficiency/2020404\n",
      "https://www.pro-football-reference.com/years/2022//stats/nfl/special-teams/2020404\n"
     ]
    }
   ],
   "source": [
    "tablenames = ['/stats/nfl/team-defense/2020','/stats/nfl/team-offense/2020', '/stats/nfl/team-efficiency/2020', '/stats/nfl/special-teams/2020' ]\n",
    "\n",
    "# assign url\n",
    "baseurl = 'https://www.footballoutsiders.com'\n",
    "\n",
    "# initiate session and login\n",
    "s = requests.Session()\n",
    "data =  {'edit-name': emailName,'edit-password':passwordName}\n",
    "login = s.post(baseurl,data=data)\n",
    "\n",
    "# iterate through the tables to create dataframes\n",
    "for tablename in tablenames:\n",
    "    \n",
    "    # assign the url\n",
    "    url = pfr + tablename\n",
    "    \n",
    "    filename = tablename\n",
    "    \n",
    "    # scrape the tables\n",
    "    scrape_table(url, tablename, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94d6fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF (tablename, path):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        filename = path +tablename + '_vegaslines.csv'\n",
    "\n",
    "        newDF = pd.read_csv(filename)\n",
    "        \n",
    "        newDF['Team'] = tablename\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(filename)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d801c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarae\\Desktop\\NFL_Modeling\\nwe_vegaslines.csv\n"
     ]
    }
   ],
   "source": [
    "# put all vegas information in one csv\n",
    "\n",
    "#recreate file names (  to make this more efficient in the future)\n",
    "tablenames = allTeams[\"pfr_abbreviation\"]\n",
    "teamAbb = allTeams[\"team_abbreviation\"]\n",
    "\n",
    "# create an empty dataframe\n",
    "vegasDF = pd.DataFrame()\n",
    "\n",
    "for tablename in tablenames:\n",
    "    \n",
    "    vegasDF = pd.concat([vegasDF, getDF(tablename, path)], axis = 0)\n",
    "    \n",
    "filepath = path +'allvegas.csv'\n",
    "\n",
    "vegasDF.to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dba4afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
